sample(c(1,2,3), 1, prob=c(1,1,1))
sample(c(1,2,3), 1, prob=c(1,1,1))
sample(c(1,2,3), 1, prob=c(1,1,1))
sample(c(1,2,3), 1, prob=c(1,1,1))
sample(c(1,2,3), 1, prob=c(1,1,1))
sample(c(1,2,3), 1, prob=c(1,0.001,0.001))
sample(c(1,2,3), 1, prob=c(1,0.001,0.001))
sample(c(1,2,3), 1, prob=c(1,0.001,0.001))
sample(c(1,2,3), 1, prob=c(1,0.001,0.001))
?sample
smap1 <- c("S")
samp1 <- c("S")
length(samp1[samp1=="I"])
library(ggplot2)
library(ggdag)
library(ggplot2)
library(ggdag)
# Create sample data
set.seed(123)
data <- data.frame(
x = rnorm(100),
y = rnorm(100),
z = rnorm(100)
)
# Create scatter plot matrix
ggplot(data, aes(x = x, y = y)) +
geom_point() +
facet_grid(vars(z))
# Create scatter plot matrix
ggplot(data, aes(x = x, y = y)) +
geom_point() +
facet_grid(vars(z))
# Create DAG
ggdag(
# Nodes
nodes = c("x", "y", "z"),
# Edges
edges = c(
"z" -> "x",
"z" -> "y",
"x" -> "y"
)
)
# Create DAG
ggdag(
# Nodes
nodes = c("x", "y", "z"),
# Edges
edges = c(
"z" -> "x",
"z" -> "y",
"x" -> "y"
)
)
# Create scatter plot matrix
ggplot(data, aes(x = x, y = y)) +
geom_point() +
facet_grid(vars(z))
library(dagitty)
# Create DAG using dagitty syntax
dag <- dagitty("dag {
z -> x
z -> y
x -> y
}")
library(ggdag)
# Create graph using ggdag
ggdag(dag)
# Create graph using ggdag
ggdag(dag) +
theme_bw()
1/0.0230
1/0.0131
1/0.3
mean(0.0215, 0.0341, 0.0179, 0.0703)
mean(c(0.0215, 0.0341, 0.0179, 0.0703))
100/(94+9)*94
1744.75+4587.75
3416.70 + 300
(3416.70 + 300)/3
version
library(stabiliser)
data("stabiliser_example")
set.seed(8141)
stable_enet <- stabilise(data = stabiliser_example,
outcome = "y")
stable_enet$enet$stability
stable_enet$enet$perm_thresh
stab_plot(stabiliser_object = stable_enet)
stab_output <- stabilise(outcome = "y", data = stabiliser_example, models = c("mbic", "enet", "mcp", "lasso"), type = "linear")
132/7.4
4 * 7.4
5 * 7.4
30+30+30+37
install.packages("neatpkg")
install.packages("neatpkg", repos=c("https://cran.rstudio.com/", "https://ku-awdc.github.io/drat/"))
library("neatpkg")
print("Muhahahaha!")
48+24
86-72
643*63
643*0.063
745.000 / 12
61305*12
53805+9200
160*2
332.160/3
960+320+3840+4800+200
200+960+960+960
200+1920+2160+200+200
200000+960000+960000+960000
120000+1152000+1296000+120000
60/12
60/24
exp(4.5)
exp(7.5)
library("Microsoft365R")
list_sharepoint_sites()
library("Microsoft365R")
list_sharepoint_sites()
site <- get_sharepoint_site("My site")
?get_sharepoint_site
mysite <- get_sharepoint_site(site_url="https://alumni.sharepoint.com/:f:/r/sites/UCPH_ENIGMA-AvianInfluenzaProject/Shared%20Documents/General/WOAH%20data%202023?csf=1&web=1&e=oFEzCM")
mysite <- get_sharepoint_site(site_url="https://alumni.sharepoint.com/f/r/sites/UCPH_ENIGMA-AvianInfluenzaProject/Shared%20Documents/General/WOAH%20data%202023?csf=1&web=1&e=oFEzCM")
mysite <- get_sharepoint_site(site_url="https://alumni.sharepoint.com/:f:/r/sites/UCPH_ENIGMA-AvianInfluenzaProject/Shared%20Documents/General/WOAH%20data%202023?csf=1&web=1&e=oFEzCM")
site <- get_sharepoint_site("UCPH_ENIGMA")
site <- get_sharepoint_site("UCPH_ENIGMA - Avian Influenza Project")
site <- get_sharepoint_site("UCPH_ENIGMA - Avian Influenza Project")
install.packages("Microsoft365R")
install.packages("Microsoft365R")
library("Microsoft365R")
list_sharepoint_sites()
list_sharepoint_sites()
?list_sharepoint_sites
site <- get_sharepoint_site("UCPH_ENIGMA - Avian Influenza Project", tenant="University of Copenhagen")
get_personal_onedrive(auth_type="device_code")
get_personal_onedrive(auth_type="device_code")
get_personal_onedrive(auth_type="device_code")
site <- get_sharepoint_site("UCPH_ENIGMA - Avian Influenza Project", tenant="University of Copenhagen")
mysite <- get_sharepoint_site(site_url="https://alumni.sharepoint.com/:f:/r/sites/UCPH_ENIGMA-AvianInfluenzaProject/Shared%20Documents/General/WOAH%20data%202023?csf=1&web=1&e=oFEzCM")
# list of all document libraries under this site
site$list_drives()
list_sharepoint_sites()
get_personal_onedrive(auth_type="device_code")
site <- get_sharepoint_site("My site")
site <- get_sharepoint_site("UCPH_ENIGMA - Avian Influenza Project")
site <- get_sharepoint_site("UCPH_ENIGMA - Avian Influenza Project", tenant="University of Copenhagen")
library("Microsoft365R")
list_sharepoint_sites()
site <- get_sharepoint_site("UCPH_ENIGMA - Avian Influenza Project", tenant="University of Copenhagen")
OBS! Hvis jeg skal bruge adgang fra en Shiny App, SKAL jeg lave en custom access:
library("Microsoft365R")
site <- get_sharepoint_site("UCPH_ENIGMA - Avian Influenza Project")
site <- get_sharepoint_site("https://teams.microsoft.com/l/team/19%3aQtMnpM2FNqjbuVR-zKKWNrkMJLx-SL40DK3XGAgl-qc1%40thread.tacv2/conversations?groupId=eb60e1fd-3c5e-4797-ad84-3dcb642b1ec0&tenantId=a3927f91-cda1-4696-af89-8c9f1ceffa91")
site <- get_sharepoint_site("https://teams.microsoft.com/l/team/19%3aQtMnpM2FNqjbuVR-zKKWNrkMJLx-SL40DK3XGAgl-qc1%40thread.tacv2/conversations?groupId=eb60e1fd-3c5e-4797-ad84-3dcb642b1ec0&tenantId=a3927f91-cda1-4696-af89-8c9f1ceffa91")
mysite <- get_sharepoint_site(site_url="https://teams.microsoft.com/l/team/19%3aQtMnpM2FNqjbuVR-zKKWNrkMJLx-SL40DK3XGAgl-qc1%40thread.tacv2/conversations?groupId=eb60e1fd-3c5e-4797-ad84-3dcb642b1ec0&tenantId=a3927f91-cda1-4696-af89-8c9f1ceffa91")
mysite <- get_sharepoint_site(site_url="https://alumni.sharepoint.com/:x:/r/sites/UCPH_ENIGMA-AvianInfluenzaProject/Shared%20Documents/General/WOAH%20data%202023/infur_20230324.xlsx?d=wc2b1ccc6b802406882a7c2a25fde6a8b&csf=1&web=1&e=SLOSka")
list_sharepoint_sites()
list_drives()
list_drives()
list_sharepoint_sites()
list_sharepoint_sites()
site <- get_sharepoint_site("My site")
mysite <- get_sharepoint_site(site_url="https://alumni.sharepoint.com/:x:/r/sites/UCPH_ENIGMA-AvianInfluenzaProject/Shared%20Documents/General/WOAH%20data%202023/infur_20230324.xlsx?d=wc2b1ccc6b802406882a7c2a25fde6a8b&csf=1&web=1&e=7VZrCU")
mysite <- get_sharepoint_site(site_url="https://alumni.sharepoint.com/:x::/r/sites/UCPH_ENIGMA-AvianInfluenzaProject/Shared%20Documents/General/WOAH%20data%202023/infur_20230324.xlsx?d=wc2b1ccc6b802406882a7c2a25fde6a8b&csf=1&web=1&e=7VZrCU")
mysite <- get_sharepoint_site(site_url="https://alumni.sharepoint.com/:x:/s/sites/UCPH_ENIGMA-AvianInfluenzaProject/Shared%20Documents/General/WOAH%20data%202023/infur_20230324.xlsx?d=wc2b1ccc6b802406882a7c2a25fde6a8b&csf=1&web=1&e=7VZrCU")
tmp <- https://alumni.sharepoint.com/:x:/r/sites/UCPH_ENIGMA-AvianInfluenzaProject/Shared%20Documents/General/WOAH%20data%202023/infur_20230407.xlsx?d=waa24f89a788147e7a092bc6522d90ddd&csf=1&web=1&e=mcjUqu
mysite <- get_sharepoint_site(site_url=tmp)
tmp <- "https://alumni.sharepoint.com/:x:/r/sites/UCPH_ENIGMA-AvianInfluenzaProject/Shared%20Documents/General/WOAH%20data%202023/infur_20230407.xlsx?d=waa24f89a788147e7a092bc6522d90ddd&csf=1&web=1&e=mcjUqu"
mysite <- get_sharepoint_site(site_url=tmp)
library("Microsoft365R")
list_sharepoint_sites()
tmp <- "https://alumni.sharepoint.com/sites/UCPH_ENIGMA-AvianInfluenzaProject/Shared%20Documents/General/WOAH%20data%202023/infur_20230407.xlsx?d=waa24f89a788147e7a092bc6522d90ddd&csf=1&web=1&e=mcjUqu"
mysite <- get_sharepoint_site(site_url=tmp)
tmp <- "https://alumni.sharepoint.com/:x:/r/sites/UCPH_ENIGMA-AvianInfluenzaProject/Shared%20Documents/General/WOAH%20data%202023/infur_20230407.xlsx?d=waa24f89a788147e7a092bc6522d90ddd&csf=1&web=1&e=mcjUqu"
mysite <- get_sharepoint_site(site_url=tmp)
library("Microsoft365R")
list_sharepoint_sites()
# default document library
drv <- site$get_drive()
# list of all document libraries under this site
site$list_drives()
site <- get_sharepoint_site("UCPH_ENIGMA - Avian Influenza Project", tenant="University of Copenhagen")
site <- get_sharepoint_site("UCPH_ENIGMA - Avian Influenza Project", tenant="alumni")
list_sharepoint_sites()
site <- get_sharepoint_site("My site")
mysite <- get_sharepoint_site("My site", tenant="alumni")
site <- get_sharepoint_site("UCPH_ENIGMA-AvianInfluenzaProject", tenant="alumni")
site <- get_sharepoint_site("UCPH_ENIGMA-AvianInfluenzaProject", tenant="alumni")
library(readxl)
url <- "https://alumni.sharepoint.com/:x:/r/sites/UCPH_ENIGMA-AvianInfluenzaProject/Shared%20Documents/General/WOAH%20data%202023/infur_20230407.xlsx"
download.file(url = url, destfile = "C:/file.xlsx", mode = "wb")
tmp <- "https://alumni.sharepoint.com/:x:/r/sites/UCPH_ENIGMA-AvianInfluenzaProject/Shared%20Documents/General/WOAH%20data%202023/infur_20230407.xlsx"
mysite <- get_sharepoint_site(site_url=tmp)
tmp <- "https://alumni.sharepoint.com/:x:/r/sites/UCPH_ENIGMA-AvianInfluenzaProject/Shared%20Documents/General/WOAH%20data%202023/infur_20230407.xlsx"
mysite <- get_sharepoint_site(site_url=tmp)
library("Microsoft365R")
list_sharepoint_sites()
400*24
160*3*8
3*8
160*3*8
160*3*6
332.160/960
335.160/840
2080*346
719.680 + 319.200
719680 + 319200
960+320+3840+2880+200
90*5
#install.packages("Microsoft365R")
library("Microsoft365R")
list_sharepoint_sites()
tmp <- "https://alumni.sharepoint.com/:x:/r/sites/UCPH_ENIGMA-AvianInfluenzaProject/Shared%20Documents/General/WOAH%20data%202023/infur_20230407.xlsx"
mysite <- get_sharepoint_site(site_url=tmp)
devtools::install_github("esbeneickhardt/sharepointr")
# Setting parameters
client_id <- "insert_from_first_step"
# Setting parameters
client_id <- "insert_from_first_step"
client_secret <- "insert_from_first_step"
tenant_id <- "insert_from_fourth_step"
resource_id <- "insert_from_fourth_step"
site_domain <- "yourorganisation.sharepoint.com"
#sharepoint_url <- "https://yourorganisation.sharepoint.com/sites/MyTestSite"
sharepoint_url <- "https://alumni.sharepoint.com/:x:/r/sites/UCPH_ENIGMA-AvianInfluenzaProject/Shared%20Documents/General/WOAH%20data%202023/_layouts/15/appregnew.aspx"
"https://alumni.sharepoint.com/:x:/r/sites/UCPH_ENIGMA-AvianInfluenzaProject/Shared%20Documents/General/WOAH%20data%202023/https://yourorganisation.sharepoint.com/sites/MyTestSite/_layouts/15/appregnew.aspx  "
# Getting a SharePoint Token
sharepoint_token <- get_sharepoint_token(client_id, client_secret, tenant_id, resource_id, site_domain)
site_domain <- "alumni.sharepoint.com"
#sharepoint_url <- "https://yourorganisation.sharepoint.com/sites/MyTestSite"
sharepoint_url <- "https://alumni.sharepoint.com/:x:/r/sites/UCPH_ENIGMA-AvianInfluenzaProject/Shared%20Documents/General/WOAH%20data%202023/_layouts/15/appregnew.aspx"
# Getting a SharePoint Token
sharepoint_token <- get_sharepoint_token(client_id, client_secret, tenant_id, resource_id, site_domain)
# Getting sharepoint digest value
sharepoint_digest_value <- get_sharepoint_digest_value(sharepoint_token, sharepoint_url)
# Installing the package
#install.packages("devtools")
devtools::install_github("esbeneickhardt/sharepointr")
install.packages("devtools")
install.packages("devtools")
devtools::install_github("esbeneickhardt/sharepointr")
# Setting parameters
client_id <- "insert_from_first_step"
client_secret <- "insert_from_first_step"
tenant_id <- "insert_from_fourth_step"
resource_id <- "insert_from_fourth_step"
site_domain <- "alumni.sharepoint.com"
#sharepoint_url <- "https://yourorganisation.sharepoint.com/sites/MyTestSite"
sharepoint_url <- "https://alumni.sharepoint.com/:x:/r/sites/UCPH_ENIGMA-AvianInfluenzaProject/Shared%20Documents/General/WOAH%20data%202023/_layouts/15/appregnew.aspx"
# Getting a SharePoint Token
sharepoint_token <- get_sharepoint_token(client_id, client_secret, tenant_id, resource_id, site_domain)
#install.packages("Microsoft365R")
library("Microsoft365R")
library(sharepointr)
# Setting parameters
client_id <- "insert_from_first_step"
client_secret <- "insert_from_first_step"
tenant_id <- "insert_from_fourth_step"
resource_id <- "insert_from_fourth_step"
site_domain <- "alumni.sharepoint.com"
#sharepoint_url <- "https://yourorganisation.sharepoint.com/sites/MyTestSite"
sharepoint_url <- "https://alumni.sharepoint.com/:x:/r/sites/UCPH_ENIGMA-AvianInfluenzaProject/Shared%20Documents/General/WOAH%20data%202023/_layouts/15/appregnew.aspx"
"https://alumni.sharepoint.com/:x:/r/sites/UCPH_ENIGMA-AvianInfluenzaProject/Shared%20Documents/General/WOAH%20data%202023/https://yourorganisation.sharepoint.com/sites/MyTestSite/_layouts/15/appregnew.aspx  "
# Getting a SharePoint Token
sharepoint_token <- get_sharepoint_token(client_id, client_secret, tenant_id, resource_id, site_domain)
sharepoint_token
client_id
#install.packages("Microsoft365R")
library("Microsoft365R")
list_sharepoint_sites()
list_sharepoint_sites()
site <- get_sharepoint_site("My site")
mysite <- get_sharepoint_site("My site", tenant="mycompany")
tmp <- "https://alumni.sharepoint.com/:x:/r/sites/UCPH_ENIGMA-AvianInfluenzaProject/Shared%20Documents/General/WOAH%20data%202023/infur_20230407.xlsx"
mysite <- get_sharepoint_site(site_url=tmp)
site <- get_sharepoint_site("UCPH_ENIGMA-AvianInfluenzaProject", tenant="alumni")
mysite <- get_sharepoint_site(site_url="")
url <- "https://alumni.sharepoint.com/:x:/r/sites/UCPH_ENIGMA-AvianInfluenzaProject/Shared%20Documents/General/WOAH%20data%202023/infur_20230407.xlsx"
download.file(url = url, destfile = "C:/file.xlsx", mode = "wb")
read_excel("D:/file.xlsx")
mysite <- get_sharepoint_site(site_url=tmp)
site <- get_sharepoint_site("https://teams.microsoft.com/l/team/19%3aQtMnpM2FNqjbuVR-zKKWNrkMJLx-SL40DK3XGAgl-qc1%40thread.tacv2/conversations?groupId=eb60e1fd-3c5e-4797-ad84-3dcb642b1ec0&tenantId=a3927f91-cda1-4696-af89-8c9f1ceffa91")
site <- get_sharepoint_site("My site")
mysite <- get_sharepoint_site(site_url="")
mysite <- get_sharepoint_site("My site", tenant="mycompany")
60/12
install.packages("seraphim")
version
hh
## Copyright © 2023 Lene J. Kjær, Michael P. Ward, Anette E. Boklund, Lars E. Larsen, Charlotte K. Hjulsager, and Carsten T. Kirkeby ”
##############################################################################################
# This file is part of the Shiny app for the ENIGMA HPAI model version 1.0.
# The ENIGMA HPAI model is free software: you can redistribute it and/or modify it under the terms of the GNU General Public License as published by the Free Software Foundation, either version 3 of the License, or (at your option) any later version.
# The ENIGMA HPAI model is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU General Public License for more details.
# You should have received a copy of the GNU General Public License along with the ENIGMA HPAI model. If not, see <https://www.gnu.org/licenses/>.
##############################################################################################
# Load a bunch of libraries for preformatting the code:
library(tidyverse)
library(sf)
library(readxl)
library(surveillance)
library(shiny)
library(shinydashboard)
library(countrycode)
library(tsibble)
library(ggplot2)
library(ggpubr)
library(spdep)
library(fanplot)
library(qs)
####### 1) Download seneste infur fil til denne folder: ############
#setwd("C:/ENIGMAdata/")
setwd("C:/Users/zxd598/Documents/GitHub/ku-awdc.github.io/ENIGMA2023/")
# Find the file:
ff <- list.files(pattern="infur")
# Define filename:
filename <- ff[which.max(file.info(ff)[,"mtime"])]
# Obs! filename cannot contain stuff like (1)!...
# Safety first:
stopifnot(length(filename)==1)
# Extract just the filename:
sheetname <-basename(filename) |>
stringr::str_extract("(.*)\\.\\w+", group = 1)
# Read in the data:
ai_data <- readxl::read_excel(filename, sheet=sheetname)
############################ From Lenes script #################################
### RUN THE CUSTOM FUNCTIONS SCRIPT ###
source('./src/ENIGMA_custom_functions.R')
### RUN THE DATA PREPARATION SCRIPT ###
source('./src/ENIGMA_DataPrep.R')
# get week and year for the last date of data, to create a yearweek variable to be used later in the shiny app
endDataWeek <-as.numeric(strftime(endDate, format = '%V'))
endDataYear <-as.numeric(strftime(endDate, format = '%Y'))
europe_data_weekly$week <- paste0('W', europe_data_weekly$Week)
europe_data_weekly$yearweek <- yearweek(paste0(europe_data_weekly$Year, ' ', europe_data_weekly$week))
#subset data and covariates - to start in week 39 of 2021
subset_start <- 299
subset_end <- nrow(AI_weekly)
subset_trainTest<- 312 + (endYear-2021-1)*52+ endWeek
AI_weekly1 <-  AI_weekly[subset_start:subset_end,,drop=FALSE]
AI_wet1 <-  AI_wet[subset_start:subset_end,,drop=FALSE]
AI_coast1 <-  AI_coast[subset_start:subset_end,,drop=FALSE]
start_W <- 39
start_Y <- 2021
# set min and max date of data for shiny app
mindate <- '2021-09-27'
maxdate<- endDate
### CONSTRUCTION OF CLASS STS USED IN hhh4 MODELS ###
#Parameters in the sts object are created in the Data preparation script. Observed are the counts, start is start year and sample number, frequency is number of observations per year (here weekly), and neighborhood is based on adjacency calculations of spatial polygons (here europeanCountries.sub):
AI_sts <- sts(observed = AI_weekly1, start = c(as.numeric(start_Y), as.numeric(start_W)),
frequency = 52,neighbourhood = europe_adjmat,
map = europeanCountries.sub)
# here we set the training data set - which is basically the whole data starting at the second week, as we already validated the model earlier
TRAIN <- 2:(subset_trainTest-subset_start +1)
# these variables are created to keep track of country placement and country names in the AI_sts object. They are used for plotting predictions and simulations for individual countries
districts2plot<-  which(colSums(observed(AI_sts), na.rm=T) > 50)
districts2plot1 <- countrycode(as.character(names(districts2plot)),"iso3c","country.name")
names(districts2plot) <- districts2plot1
districts2plot <-districts2plot[order((names(districts2plot)))]
districts2plot <-setNames(c(districts2plot, 0), c(names(districts2plot), "Summed all countries"))
districts2plot2 <-names(districts2plot)
#area fraction of summed countries
area_frac <- country_area[subset_start:subset_end,,drop=FALSE]/rowSums(country_area[subset_start:subset_end,,drop=FALSE])
### FINAL MODEL FROM KJÆR ET AL. 2023###
#model with long distance transmission, random effects and seasonality in the epidemic component
final_model_base <- list(end = list(f = ~1 + ri(type = "iid") - 1,offset=area_frac), ar = list(f=addSeason2formula( ~ 1 +t+ ri(type = "iid") - 1, S=2, period= AI_sts@freq)),
ne = list(f = ~1 + ri(type = "iid") - 1, weights = W_powerlaw(maxlag = max(neighbourhood(AI_sts),log = TRUE,normalize = TRUE, from0 = TRUE))),
family = "NegBin1",optimizer = list(stop = list(tol=1e-5, niter=500),
regression = list(method="nlminb"),
variance = list(method="Nelder-Mead")),subset = TRAIN, keep.terms = TRUE)
final_model <-hhh4(stsObj = AI_sts,control = final_model_base)
# Save the date:
save_date <- Sys.Date()
# Save the data in an (arbitrarily names file format) ".car"  format:
# We set the wd temporarily to tmp, to save a ".car" file:
setwd("C:/Users/zxd598/Documents/GitHub/ku-awdc.github.io/ENIGMA2023/tmp")
qs::qsave(list(ai_data=ai_data,
save_date=save_date,
filename=filename,
endDate=endDate,
updateDate=updateDate,
europe_data_weekly=europe_data_weekly,
AI_weekly=AI_weekly,
AI_wet=AI_wet,
AI_coast=AI_coast,
europe_adjmat=europe_adjmat,
europeanCountries.sub=europeanCountries.sub,
europeanCountries=europeanCountries,
mindate=mindate,
maxdate=maxdate,
districts2plot=districts2plot,
districts2plot2=districts2plot2,
AI_sts=AI_sts,
final_model=final_model),
file="for_ai.car", preset="archive")
file.copy(file.path(getwd(), "for_ai.car"), "C:/Users/zxd598/Documents/GitHub/ku-awdc.github.io/ENIGMA2023/for_ai.car", overwrite=TRUE)
# Clean up:
rm(ai_data)
## Copyright © 2023 Lene J. Kjær, Michael P. Ward, Anette E. Boklund, Lars E. Larsen, Charlotte K. Hjulsager, and Carsten T. Kirkeby ”
##############################################################################################
# This file is part of the Shiny app for the ENIGMA HPAI model version 1.0.
# The ENIGMA HPAI model is free software: you can redistribute it and/or modify it under the terms of the GNU General Public License as published by the Free Software Foundation, either version 3 of the License, or (at your option) any later version.
# The ENIGMA HPAI model is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU General Public License for more details.
# You should have received a copy of the GNU General Public License along with the ENIGMA HPAI model. If not, see <https://www.gnu.org/licenses/>.
##############################################################################################
# Load a bunch of libraries for preformatting the code:
library(tidyverse)
library(sf)
library(readxl)
library(surveillance)
library(shiny)
library(shinydashboard)
library(countrycode)
library(tsibble)
library(ggplot2)
library(ggpubr)
library(spdep)
library(fanplot)
library(qs)
####### 1) Download seneste infur fil til denne folder: ############
#setwd("C:/ENIGMAdata/")
setwd("C:/Users/zxd598/Documents/GitHub/ku-awdc.github.io/ENIGMA2023/")
# Find the file:
ff <- list.files(pattern="infur")
# Define filename:
filename <- ff[which.max(file.info(ff)[,"mtime"])]
# Obs! filename cannot contain stuff like (1)!...
# Safety first:
stopifnot(length(filename)==1)
# Extract just the filename:
sheetname <-basename(filename) |>
stringr::str_extract("(.*)\\.\\w+", group = 1)
# Read in the data:
ai_data <- readxl::read_excel(filename, sheet=sheetname)
############################ From Lenes script #################################
### RUN THE CUSTOM FUNCTIONS SCRIPT ###
source('./src/ENIGMA_custom_functions.R')
### RUN THE DATA PREPARATION SCRIPT ###
source('./src/ENIGMA_DataPrep.R')
### NOW RUN MODEL BASED ON THR LATEST DATA, BUT STARTING IN WEEK 39 IN  2021
# get week and year for the last date of data, to create a yearweek variable to be used later in the shiny app
endDataWeek <-as.numeric(strftime(endDate, format = '%V'))
endDataYear <-as.numeric(strftime(endDate, format = '%Y'))
europe_data_weekly$week <- paste0('W', europe_data_weekly$Week)
europe_data_weekly$yearweek <- yearweek(paste0(europe_data_weekly$Year, ' ', europe_data_weekly$week))
#subset data and covariates - to start in week 39 of 2021
subset_start <- 299
subset_end <- nrow(AI_weekly)
subset_trainTest<- 312 + (endYear-2021-1)*52+ endWeek
AI_weekly1 <-  AI_weekly[subset_start:subset_end,,drop=FALSE]
AI_wet1 <-  AI_wet[subset_start:subset_end,,drop=FALSE]
AI_coast1 <-  AI_coast[subset_start:subset_end,,drop=FALSE]
start_W <- 39
start_Y <- 2021
# set min and max date of data for shiny app
mindate <- '2021-09-27'
maxdate<- endDate
### CONSTRUCTION OF CLASS STS USED IN hhh4 MODELS ###
#Parameters in the sts object are created in the Data preparation script. Observed are the counts, start is start year and sample number, frequency is number of observations per year (here weekly), and neighborhood is based on adjacency calculations of spatial polygons (here europeanCountries.sub):
AI_sts <- sts(observed = AI_weekly1, start = c(as.numeric(start_Y), as.numeric(start_W)),
frequency = 52,neighbourhood = europe_adjmat,
map = europeanCountries.sub)
# here we set the training data set - which is basically the whole data starting at the second week, as we already validated the model earlier
TRAIN <- 2:(subset_trainTest-subset_start +1)
# these variables are created to keep track of country placement and country names in the AI_sts object. They are used for plotting predictions and simulations for individual countries
districts2plot<-  which(colSums(observed(AI_sts), na.rm=T) > 50)
districts2plot1 <- countrycode(as.character(names(districts2plot)),"iso3c","country.name")
names(districts2plot) <- districts2plot1
districts2plot <-districts2plot[order((names(districts2plot)))]
districts2plot <-setNames(c(districts2plot, 0), c(names(districts2plot), "Summed all countries"))
districts2plot2 <-names(districts2plot)
#area fraction of summed countries
area_frac <- country_area[subset_start:subset_end,,drop=FALSE]/rowSums(country_area[subset_start:subset_end,,drop=FALSE])
### FINAL MODEL FROM KJÆR ET AL. 2023###
#model with long distance transmission, random effects and seasonality in the epidemic component
final_model_base <- list(end = list(f = ~1 + ri(type = "iid") - 1,offset=area_frac), ar = list(f=addSeason2formula( ~ 1 +t+ ri(type = "iid") - 1, S=2, period= AI_sts@freq)),
ne = list(f = ~1 + ri(type = "iid") - 1, weights = W_powerlaw(maxlag = max(neighbourhood(AI_sts),log = TRUE,normalize = TRUE, from0 = TRUE))),
family = "NegBin1",optimizer = list(stop = list(tol=1e-5, niter=500),
regression = list(method="nlminb"),
variance = list(method="Nelder-Mead")),subset = TRAIN, keep.terms = TRUE)
final_model <-hhh4(stsObj = AI_sts,control = final_model_base)
################################################################################
# Save the date:
save_date <- Sys.Date()
# Save the data in an (arbitrarily names file format) ".car"  format:
# We set the wd temporarily to tmp, to save a ".car" file:
setwd("C:/Users/zxd598/Documents/GitHub/ku-awdc.github.io/ENIGMA2023/tmp")
qs::qsave(list(ai_data=ai_data,
save_date=save_date,
filename=filename,
endDate=endDate,
updateDate=updateDate,
europe_data_weekly=europe_data_weekly,
AI_weekly=AI_weekly,
AI_wet=AI_wet,
AI_coast=AI_coast,
europe_adjmat=europe_adjmat,
europeanCountries.sub=europeanCountries.sub,
europeanCountries=europeanCountries,
mindate=mindate,
maxdate=maxdate,
districts2plot=districts2plot,
districts2plot2=districts2plot2,
AI_sts=AI_sts,
final_model=final_model),
file="for_ai.car", preset="archive")
file.copy(file.path(getwd(), "for_ai.car"), "C:/Users/zxd598/Documents/GitHub/ku-awdc.github.io/ENIGMA2023/for_ai.car", overwrite=TRUE)
# Clean up:
rm(ai_data)
############# 2) Push to git ###############
#### Procedure: ####
